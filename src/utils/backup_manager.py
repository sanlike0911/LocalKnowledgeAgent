"""
ChromaDBãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç®¡ç†ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å®šæœŸçš„ãªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’æä¾›
"""

import os
import json
import shutil
import zipfile
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import logging


@dataclass
class BackupInfo:
    """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±"""
    backup_path: str
    timestamp: datetime
    backup_type: str  # 'full', 'incremental'
    file_count: int
    size_bytes: int
    status: str  # 'success', 'partial', 'failed'
    error_message: Optional[str] = None


class BackupManager:
    """
    ChromaDBã¨è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç®¡ç†ã‚¯ãƒ©ã‚¹
    
    æ©Ÿèƒ½:
    - ChromaDBãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å…¨ä½“ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    - è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    - ä¸–ä»£ç®¡ç†ï¼ˆå¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®è‡ªå‹•å‰Šé™¤ï¼‰
    - å¢—åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚µãƒãƒ¼ãƒˆ
    """
    
    def __init__(
        self,
        backup_dir: str = "./backups",
        max_backups: int = 10,
        retention_days: int = 30
    ):
        """
        ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–
        
        Args:
            backup_dir: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            max_backups: ä¿æŒã™ã‚‹æœ€å¤§ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ•°
            retention_days: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿æŒæ—¥æ•°
        """
        self.backup_dir = Path(backup_dir)
        self.max_backups = max_backups
        self.retention_days = retention_days
        self.logger = logging.getLogger(__name__)
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«
        self.backup_info_file = self.backup_dir / "backup_info.json"
        
        self.logger.info(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–å®Œäº†: {backup_dir}")
    
    def create_backup(
        self,
        chroma_db_path: str,
        config_file_path: str,
        backup_type: str = "full"
    ) -> BackupInfo:
        """
        ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ
        
        Args:
            chroma_db_path: ChromaDBãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
            config_file_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            backup_type: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¿ã‚¤ãƒ— ('full' ã¾ãŸã¯ 'incremental')
            
        Returns:
            BackupInfo: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±
        """
        timestamp = datetime.now()
        backup_name = f"backup_{timestamp.strftime('%Y%m%d_%H%M%S')}"
        backup_path = self.backup_dir / f"{backup_name}.zip"
        
        try:
            self.logger.info(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆé–‹å§‹: {backup_name}")
            
            file_count = 0
            size_bytes = 0
            
            with zipfile.ZipFile(backup_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # ChromaDBãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
                if Path(chroma_db_path).exists():
                    db_files = self._get_database_files(chroma_db_path)
                    for file_path in db_files:
                        if Path(file_path).exists():
                            arcname = f"chroma_db/{Path(file_path).relative_to(chroma_db_path)}"
                            zipf.write(file_path, arcname)
                            file_count += 1
                            size_bytes += Path(file_path).stat().st_size
                
                # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
                if Path(config_file_path).exists():
                    zipf.write(config_file_path, "config.json")
                    file_count += 1
                    size_bytes += Path(config_file_path).stat().st_size
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
                metadata = {
                    "backup_type": backup_type,
                    "timestamp": timestamp.isoformat(),
                    "chroma_db_path": chroma_db_path,
                    "config_file_path": config_file_path,
                    "file_count": file_count,
                    "size_bytes": size_bytes
                }
                
                zipf.writestr("backup_metadata.json", json.dumps(metadata, indent=2))
            
            backup_info = BackupInfo(
                backup_path=str(backup_path),
                timestamp=timestamp,
                backup_type=backup_type,
                file_count=file_count,
                size_bytes=size_bytes,
                status="success"
            )
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±ã‚’è¨˜éŒ²
            self._save_backup_info(backup_info)
            
            # å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self._cleanup_old_backups()
            
            self.logger.info(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆæˆåŠŸ: {backup_name} ({file_count}ãƒ•ã‚¡ã‚¤ãƒ«, {size_bytes}ãƒã‚¤ãƒˆ)")
            return backup_info
            
        except Exception as e:
            error_msg = f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆã‚¨ãƒ©ãƒ¼: {e}"
            self.logger.error(error_msg)
            
            # å¤±æ•—ã—ãŸå ´åˆã¯éƒ¨åˆ†çš„ãªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            if backup_path.exists():
                backup_path.unlink()
            
            return BackupInfo(
                backup_path=str(backup_path),
                timestamp=timestamp,
                backup_type=backup_type,
                file_count=0,
                size_bytes=0,
                status="failed",
                error_message=error_msg
            )
    
    def restore_backup(
        self,
        backup_path: str,
        target_chroma_db_path: str,
        target_config_path: str
    ) -> bool:
        """
        ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒ
        
        Args:
            backup_path: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            target_chroma_db_path: å¾©å…ƒå…ˆChromaDBãƒ‘ã‚¹
            target_config_path: å¾©å…ƒå…ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            bool: å¾©å…ƒæˆåŠŸãƒ•ãƒ©ã‚°
        """
        try:
            self.logger.info(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾©å…ƒé–‹å§‹: {backup_path}")
            
            backup_path = Path(backup_path)
            if not backup_path.exists():
                raise FileNotFoundError(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {backup_path}")
            
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            self._backup_existing_data(target_chroma_db_path, target_config_path)
            
            with zipfile.ZipFile(backup_path, 'r') as zipf:
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç¢ºèª
                if "backup_metadata.json" in zipf.namelist():
                    metadata_content = zipf.read("backup_metadata.json")
                    metadata = json.loads(metadata_content.decode())
                    self.logger.info(f"å¾©å…ƒå¯¾è±¡: {metadata.get('backup_type')} ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—")
                
                # ChromaDBå¾©å…ƒ
                target_db_dir = Path(target_chroma_db_path)
                if target_db_dir.exists():
                    shutil.rmtree(target_db_dir)
                target_db_dir.mkdir(parents=True, exist_ok=True)
                
                for file_path in zipf.namelist():
                    if file_path.startswith("chroma_db/"):
                        extract_path = target_db_dir / Path(file_path).relative_to("chroma_db")
                        extract_path.parent.mkdir(parents=True, exist_ok=True)
                        
                        with zipf.open(file_path) as source, open(extract_path, 'wb') as target:
                            shutil.copyfileobj(source, target)
                
                # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å¾©å…ƒ
                if "config.json" in zipf.namelist():
                    config_content = zipf.read("config.json")
                    target_config = Path(target_config_path)
                    target_config.parent.mkdir(parents=True, exist_ok=True)
                    
                    with open(target_config, 'wb') as f:
                        f.write(config_content)
            
            self.logger.info(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾©å…ƒæˆåŠŸ: {backup_path}")
            return True
            
        except Exception as e:
            error_msg = f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}"
            self.logger.error(error_msg)
            return False
    
    def list_backups(self) -> List[BackupInfo]:
        """
        åˆ©ç”¨å¯èƒ½ãªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸€è¦§ã‚’å–å¾—
        
        Returns:
            List[BackupInfo]: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±ãƒªã‚¹ãƒˆ
        """
        backup_info = self._load_backup_info()
        return sorted(backup_info, key=lambda x: x.timestamp, reverse=True)
    
    def get_backup_status(self) -> Dict:
        """
        ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ ã®çŠ¶æ…‹ã‚’å–å¾—
        
        Returns:
            Dict: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—çŠ¶æ…‹æƒ…å ±
        """
        backups = self.list_backups()
        
        total_size = sum(backup.size_bytes for backup in backups)
        successful_backups = [b for b in backups if b.status == "success"]
        failed_backups = [b for b in backups if b.status == "failed"]
        
        latest_backup = backups[0] if backups else None
        
        return {
            "total_backups": len(backups),
            "successful_backups": len(successful_backups),
            "failed_backups": len(failed_backups),
            "total_size_bytes": total_size,
            "total_size_human": self._format_size(total_size),
            "latest_backup": {
                "timestamp": latest_backup.timestamp.isoformat() if latest_backup else None,
                "status": latest_backup.status if latest_backup else None,
                "size": self._format_size(latest_backup.size_bytes) if latest_backup else None
            } if latest_backup else None,
            "backup_dir": str(self.backup_dir),
            "max_backups": self.max_backups,
            "retention_days": self.retention_days
        }
    
    def _get_database_files(self, db_path: str) -> List[str]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—"""
        db_path = Path(db_path)
        if not db_path.exists():
            return []
        
        db_files = []
        for root, dirs, files in os.walk(db_path):
            for file in files:
                file_path = Path(root) / file
                db_files.append(str(file_path))
        
        return db_files
    
    def _save_backup_info(self, backup_info: BackupInfo) -> None:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±ã‚’ä¿å­˜"""
        existing_info = self._load_backup_info()
        existing_info.append(backup_info)
        
        # JSONå½¢å¼ã§ä¿å­˜
        backup_data = []
        for info in existing_info:
            backup_data.append({
                "backup_path": info.backup_path,
                "timestamp": info.timestamp.isoformat(),
                "backup_type": info.backup_type,
                "file_count": info.file_count,
                "size_bytes": info.size_bytes,
                "status": info.status,
                "error_message": info.error_message
            })
        
        with open(self.backup_info_file, 'w', encoding='utf-8') as f:
            json.dump(backup_data, f, indent=2, ensure_ascii=False)
    
    def _load_backup_info(self) -> List[BackupInfo]:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±ã‚’èª­ã¿è¾¼ã¿"""
        if not self.backup_info_file.exists():
            return []
        
        try:
            with open(self.backup_info_file, 'r', encoding='utf-8') as f:
                backup_data = json.load(f)
            
            backup_info = []
            for data in backup_data:
                backup_info.append(BackupInfo(
                    backup_path=data["backup_path"],
                    timestamp=datetime.fromisoformat(data["timestamp"]),
                    backup_type=data["backup_type"],
                    file_count=data["file_count"],
                    size_bytes=data["size_bytes"],
                    status=data["status"],
                    error_message=data.get("error_message")
                ))
            
            return backup_info
            
        except Exception as e:
            self.logger.warning(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _cleanup_old_backups(self) -> None:
        """å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤"""
        backups = self.list_backups()
        
        # æœ€å¤§æ•°åˆ¶é™
        if len(backups) > self.max_backups:
            old_backups = backups[self.max_backups:]
            for backup in old_backups:
                self._delete_backup(backup)
        
        # ä¿æŒæœŸé–“åˆ¶é™
        cutoff_date = datetime.now() - timedelta(days=self.retention_days)
        expired_backups = [b for b in backups if b.timestamp < cutoff_date]
        
        for backup in expired_backups:
            self._delete_backup(backup)
    
    def _delete_backup(self, backup_info: BackupInfo) -> None:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤"""
        try:
            backup_path = Path(backup_info.backup_path)
            if backup_path.exists():
                backup_path.unlink()
                self.logger.info(f"å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤: {backup_path.name}")
                
                # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±ã‹ã‚‰ã‚‚å‰Šé™¤
                existing_info = self._load_backup_info()
                existing_info = [b for b in existing_info if b.backup_path != backup_info.backup_path]
                
                # æ›´æ–°ã•ã‚ŒãŸãƒªã‚¹ãƒˆã‚’ä¿å­˜
                backup_data = []
                for info in existing_info:
                    backup_data.append({
                        "backup_path": info.backup_path,
                        "timestamp": info.timestamp.isoformat(),
                        "backup_type": info.backup_type,
                        "file_count": info.file_count,
                        "size_bytes": info.size_bytes,
                        "status": info.status,
                        "error_message": info.error_message
                    })
                
                with open(self.backup_info_file, 'w', encoding='utf-8') as f:
                    json.dump(backup_data, f, indent=2, ensure_ascii=False)
                    
        except Exception as e:
            self.logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _backup_existing_data(self, chroma_db_path: str, config_path: str) -> None:
        """å¾©å…ƒå‰ã«æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            temp_backup_name = f"temp_backup_before_restore_{timestamp}.zip"
            temp_backup_path = self.backup_dir / temp_backup_name
            
            with zipfile.ZipFile(temp_backup_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # æ—¢å­˜ChromaDBãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
                if Path(chroma_db_path).exists():
                    db_files = self._get_database_files(chroma_db_path)
                    for file_path in db_files:
                        if Path(file_path).exists():
                            arcname = f"chroma_db/{Path(file_path).relative_to(chroma_db_path)}"
                            zipf.write(file_path, arcname)
                
                # æ—¢å­˜è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
                if Path(config_path).exists():
                    zipf.write(config_path, "config.json")
                
            self.logger.info(f"å¾©å…ƒå‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {temp_backup_name}")
            
        except Exception as e:
            self.logger.warning(f"å¾©å…ƒå‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def _format_size(self, size_bytes: int) -> str:
        """ãƒã‚¤ãƒˆæ•°ã‚’äººé–“ãŒèª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›"""
        if size_bytes == 0:
            return "0 B"
        
        units = ["B", "KB", "MB", "GB", "TB"]
        size = float(size_bytes)
        unit_index = 0
        
        while size >= 1024.0 and unit_index < len(units) - 1:
            size /= 1024.0
            unit_index += 1
        
        return f"{size:.1f} {units[unit_index]}"


def create_scheduled_backup(
    chroma_db_path: str,
    config_file_path: str,
    backup_dir: str = "./backups"
) -> BackupInfo:
    """
    å®šæœŸãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ä¾¿åˆ©é–¢æ•°
    
    Args:
        chroma_db_path: ChromaDBãƒ‘ã‚¹
        config_file_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        backup_dir: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        
    Returns:
        BackupInfo: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—çµæœ
    """
    backup_manager = BackupManager(backup_dir=backup_dir)
    return backup_manager.create_backup(
        chroma_db_path=chroma_db_path,
        config_file_path=config_file_path,
        backup_type="full"
    )


def get_backup_recommendations(backup_status: Dict) -> List[str]:
    """
    ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—çŠ¶æ³ã«åŸºã¥ãæ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ
    
    Args:
        backup_status: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—çŠ¶æ…‹æƒ…å ±
        
    Returns:
        List[str]: æ¨å¥¨äº‹é …ãƒªã‚¹ãƒˆ
    """
    recommendations = []
    
    if backup_status["total_backups"] == 0:
        recommendations.append("âš ï¸ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å®šæœŸãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    
    if backup_status["failed_backups"] > 0:
        recommendations.append(f"âŒ {backup_status['failed_backups']}å€‹ã®å¤±æ•—ã—ãŸãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒã‚ã‚Šã¾ã™ã€‚")
    
    if backup_status["latest_backup"] and backup_status["latest_backup"]["timestamp"]:
        latest_time = datetime.fromisoformat(backup_status["latest_backup"]["timestamp"])
        days_ago = (datetime.now() - latest_time).days
        
        if days_ago > 7:
            recommendations.append(f"â° æœ€æ–°ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒ{days_ago}æ—¥å‰ã§ã™ã€‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        elif days_ago > 3:
            recommendations.append(f"ğŸ“… æœ€æ–°ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒ{days_ago}æ—¥å‰ã§ã™ã€‚è¿‘æ—¥ä¸­ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
    
    if backup_status["total_size_bytes"] > 5 * 1024 * 1024 * 1024:  # 5GB
        recommendations.append("ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚µã‚¤ã‚ºãŒå¤§ãããªã£ã¦ã„ã¾ã™ã€‚å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å‰Šé™¤ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
    
    if not recommendations:
        recommendations.append("âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚")
    
    return recommendations