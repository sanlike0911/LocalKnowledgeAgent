# システム設計仕様書 Ver. 3.0

## プロジェクト名： ローカルナレッジエージェント (MVP)

### 1. 概要

#### 1.1. プロジェクト概要

本仕様書は、「ローカルナレッジエージェント (MVP)」アプリケーションのシステム全体に関する設計を定義するものです。本システムは、機密情報を外部に送信することなく、ユーザーのローカルPC内で全ての処理が完結するスタンドアロンのデスクトップアプリケーションです。ユーザーが指定したローカル文書をナレッジベースとし、大規模言語モデル（LLM）を活用して対話形式での情報検索を実現します。

本仕様書は、AIコーディングエージェントによる自動実装を想定しており、アーキテクチャ、データフロー、UI/UX、状態管理など、開発に必要なすべての仕様を網羅しています。

**改訂履歴**  

* Ver. 2.1: AIエージェントの理解を促進するため、情報の構成を再編成。
* Ver. 2.2: 「主要機能」「ファイル・ディレクトリ構造」のセクションを追加し、構成を最適化。
* Ver. 2.3: 日本語固定回答制御機能の仕様を追加。
* Ver. 2.4: 必須となるOllamaモデルを明記。
* Ver. 2.5: 埋め込みモデル設定機能をワイヤフレーム・コンポーネント仕様に追加。
* Ver. 2.6: LLMモデル選択をコンボボックス化し、Ollama APIからの動的取得仕様を追加。
* Ver. 2.7: 実装完了に伴い、Phase 6機能の設計・実装同期を実施。ファイル構造・エラーハンドリング・ナビゲーション仕様を実装に合わせて更新。
* Ver. 2.8: 対応ドキュメント形式を拡張（MD, Office系ファイル）。チャット入力補助機能（プロンプト履歴表示）を追加。
* Ver. 2.9: 埋め込みモデル動的次元数対応機能を追加。モデル変更時の自動コレクション再作成機能を実装。
* Ver. 3.0: OLLAMA起動時モデル一覧取得機能、デフォルト埋め込みモデル必須チェック機能、設定画面の動的埋め込みモデルフィルタリング機能を追加。

#### 1.2. 主要機能

本アプリケーションがMVP（Minimum Viable Product）として提供する主要な機能は以下の通りです。

* **対話形式によるナレッジ検索:** 自然言語での質問に対し、登録された社内文書の内容を基に要約された回答を生成します（RAG機能）。
* **ローカル完結処理:** 全てのデータ処理（文書のインデックス作成、LLMによる回答生成など）がユーザーのPC内で完結し、機密情報が外部に送信されることはありません。
* **対象文書の管理:** ユーザーはナレッジベースとして使用するフォルダをGUIから指定できます。対応形式は **PDF, TXT, Markdown (.md), Word (.docx), Excel (.xlsx), PowerPoint (.pptx)** です。
* **モデル設定の柔軟性:** LLMモデルと埋め込みモデルを設定画面から選択・変更でき、Ollamaからインストール済みモデルを動的取得して表示します。埋め込みモデル変更時は次元数の違いを自動検出し、必要に応じてコレクションを再作成します。
* **出典の明示:** 生成された回答には、根拠となった文書のファイル名が必ず併記され、情報のトレーサビリティを確保します。
* **手動インデックス更新:** ユーザーは任意のタイミングで、ナレッジベースのインデックスを最新の状態に更新できます。
* **処理の中断:** 時間のかかる処理（Q&A応答生成、インデックス作成）をユーザーが任意に中断できる機能を備えます。
* **日本語固定回答制御:** 全ての質問に対して日本語で回答を生成するよう制御し、一貫性のあるユーザー体験を提供します。
* **チャット入力補助機能:** 質問入力ボックスでキーボードの上矢印キーを押すと、過去に入力した質問が順番に表示され、再利用が可能です。

---

### 2. システムアーキテクチャ

#### 2.1. アーキテクチャ概要

本システムのアーキテクチャは、UI層、バックエンドロジック層、データストア層、LLM実行環境の4つの主要な層で構成され、全てのコンポーネントはユーザーのローカルPC内で動作します。

* **UI層 (Frontend):** **Streamlit** を使用し、ユーザーとのインタラクション、状態の表示、バックエンド処理のトリガーを担当します。
* **バックエンドロジック層 (Backend Logic):** **LangChain** をフレームワークの中心に据え、RAG (検索拡張生成) パイプライン全体の制御を行います。
* **データストア層 (Data Stores):** 文書ベクトルを管理する **ChromaDB**、永続化設定を保存する **config.json**、そしてアプリケーションの動的な状態を管理する **Streamlit Session State** で構成されます。
* **LLM実行環境 (LLM Runtime):** **Ollama** を使用し、ローカル環境で大規模言語モデル（LLM）を安全に実行します。

アプリケーションの状態管理は **Streamlit Session State (`st.session_state`)** が中核を担い、UI層とバックエンドロジック層の間の情報連携を司ります。これにより、Streamlitの再実行モデルの中でも状態を維持し、中断処理のような複雑なインタラクションを実現します。

#### 2.2. アーキテクチャ図

```mermaid
graph TD
    subgraph ユーザーPC (ローカル環境)
        subgraph UI層 (Frontend - Streamlit)
            A[ユーザー] <--> UI(メイン画面 / 設定画面);
            UI -- (1) ユーザー操作 (質問/更新/キャンセル) --> SS[st.session_state];
            SS -- (2) 状態をUIに反映 --> UI;
        end

        subgraph バックエンドロジック層 (Backend Logic - LangChain)
            C{アプリケーション制御ロジック};
            SS -- (3) 状態変更をトリガー --> C;
            C -- (4) RAGパイプライン実行 --> RAG;
            RAG(RAGパイプライン  Document Loader, Text Splitter, Retriever, Prompting);
            C -- (9) 処理結果/進捗を更新 --> SS;
            C -- (A) ループ内で定期的にチェック --> SS_Cancel[st.session_state  cancel_requested];
            SS_Cancel -- (B) Trueの場合 --> C{処理中断};
        end

        subgraph LLM実行環境 (LLM Runtime)
            F[LLM (Ollama)];
        end
        
        subgraph データストア層 (Data Stores)
            E[ベクトルDB  ChromaDB];
            G[文書ファイル  PDF, TXT];
            CONF[設定ファイル  config.json  LLMモデル・埋め込みモデル設定];
            EM[埋め込みモデル  nomic-embed-text/mxbai-embed-large等];
        end

        %% データフロー
        RAG -- (5) 文書読み込み --> G;
        RAG -- (6) チャンクをベクトル化 --> EM;
        EM -- (7) ベクトルとテキストを保存/検索 --> E;
        RAG -- (8) プロンプトを生成し実行 --> F;
        C -- 初期起動/設定変更 --> CONF;
    end
```

#### 2.3. コンポーネント詳細

* **UI層 (Streamlit):**
  * **役割:** ユーザーからの入力を受け付け、`st.session_state` を更新することでバックエンド処理をトリガーします。また、`st.session_state` の内容に基づいて画面（チャット履歴、処理ステータス、エラーメッセージ等）を動的に描画します。
  * **責務:** ユーザーインタフェースの提供に専念し、複雑なビジネスロジックは保持しません。

* **バックエンドロジック層 (LangChain):**
  * **役割:** Streamlitからトリガーされた処理を実行します。RAGパイプライン（文書の読み込み、分割、ベクトル化、検索、LLMへの問い合わせ）の全体を制御します。
  * **責務:** `st.session_state` を監視し、状態の変更に応じて適切な処理を実行します。インデックス作成やQ&A応答生成のような時間のかかる処理のループ内で `st.session_state['cancel_requested']` を定期的に参照し、中断要求に対応します。処理の進捗や結果は `st.session_state` に書き込むことでUI層に通知します。

* **データストア層:**
  * **ベクトルDB (ChromaDB):** 文書を分割したチャンクのテキストデータと、それに対応する数値ベクトルを保存します。高速な類似度検索機能を提供し、RAGの検索フェーズで中心的な役割を果たします。
  * **設定ファイル (config.json):** ユーザーが指定したナレッジフォルダのパスやインデックスの最終更新日時など、アプリケーションの再起動後も保持すべき情報を永続的に保存します。
  * **セッション状態 (`st.session_state`):** アプリケーション実行中の揮発的な状態（`app_state`, `cancel_requested`, `chat_history`）を管理します。UIとバックエンド間の情報の受け渡しを行うハブとして機能します。

* **LLM実行環境 (Ollama):**
  * **役割:** バックエンド層から渡されたプロンプト（ユーザーの質問と関連文書）を基に、自然言語の回答を生成します。
  * **責務:** LLMの推論処理を実行することに専念します。
  * **必須モデル:** 本アプリケーションの動作には、Ollamaに以下のモデルがインストールされている必要があります。
    * **回答生成用LLM:** `llama3:8b`
    * **埋め込み（ベクトル変換）用モデル:** `nomic-embed-text`（デフォルト）
  * **サポート埋め込みモデル:** 以下の埋め込みモデルに対応しており、設定画面から選択可能です。次元数の違いは自動検出され、モデル変更時に必要に応じてコレクションが再作成されます。
    * `nomic-embed-text`: 768次元（推奨・高品質）
    * `mxbai-embed-large`: 1024次元（高精度・大容量）
    * `all-minilm`: 384次元（軽量・高速）
    * `snowflake-arctic-embed`: 1024次元（高精度）
  * **起動時チェック・モデル一覧取得:** アプリケーションは起動時に以下の処理を実行します：
    1. Ollama APIから利用可能なモデル一覧を動的取得
    2. 必須の埋め込みモデル（`nomic-embed-text`）のインストール状況を確認
    3. 不足している場合は、インストール方法（`ollama pull nomic-embed-text`）を明記したエラーを表示
    4. 取得したモデル一覧から埋め込みモデルのみをフィルタリングして設定画面で利用

#### 2.4. データフロー

##### 2.4.1. 初回起動フロー

1. アプリケーションが起動します。
2. **Ollamaモデル環境チェック：**
   - Ollama APIから利用可能なモデル一覧を取得
   - 必須埋め込みモデル（`nomic-embed-text`）のインストール状況をチェック
   - 不足している場合はエラーメッセージを表示して起動を中断
   - 取得したモデル一覧を `st.session_state['available_models']` に保存
3. バックエンドロジックが `config.json` ファイルの存在と内容をチェックします。
4. ファイルが存在しないか、有効なフォルダパスが設定されていない場合、`st.session_state` に初期設定が必要である旨のフラグを立てます。
5. UI層が `st.session_state` を参照し、初期設定を促すメッセージを表示し、Q&A機能を無効化します。
6. ファイルが存在し設定が有効な場合は、設定内容を `st.session_state['config']` にロードします。

##### 2.4.2. インデックス作成フロー

1. ユーザーが設定画面で「今すぐ更新」ボタンをクリックします。
2. UI層が `st.session_state['app_state']` を `'processing_indexing'` に変更します。
3. バックエンドロジックが `app_state` の変更を検知し、インデックス作成プロセスを開始します。
4. **[ループ開始]** 指定されたフォルダから対象ファイル（**PDF, TXT, MD, DOCX, XLSX, PPTX**）を一つずつ読み込みます (Document Loader)。
5. 読み込んだ文書を意味のあるチャンクに分割します (Text Splitter)。
6. 各チャンクをEmbedding Modelで数値ベクトルに変換します。
7. ベクトルと元のテキストをChromaDBに保存します。
8. UI層に進捗（処理済みファイル数）を `st.session_state` 経由で通知し、UIが進捗バーを更新します。
9. **[中断チェック]** ループの各イテレーションで `st.session_state['cancel_requested']` を確認。`True` ならばループを中断し、不完全なインデックスデータを破棄して終了します。
10. **[ループ終了]** 全てのファイルの処理が完了したら、`config.json` に現在の時刻を「最終更新日時」として保存し、`st.session_state['app_state']` を `'idle'` に戻します。

##### 2.4.3. Q&Aフロー

1. ユーザーがメイン画面で質問を入力し、「送信」ボタンをクリックします。
2. UI層が `st.session_state['app_state']` を `'processing_qa'` に変更し、ユーザーの質問を `st.session_state` に保存します。
3. バックエンドロジックが `app_state` の変更を検知し、Q&Aプロセスを開始します。
4. 質問文をEmbedding Modelでベクトルに変換します。
5. **[中断チェックポイント1]** `st.session_state['cancel_requested']` を確認します。
6. 質問ベクトルを用いてChromaDBから類似度の高い文書チャンクを複数検索・取得します (Retriever)。
7. **[中断チェックポイント2]** `st.session_state['cancel_requested']` を確認します。
8. 取得した文書チャンクをコンテキストとして、質問文と組み合わせたプロンプトを生成します。このプロンプトには「日本語で回答してください。」という指示を含めます。
9. 生成したプロンプト（日本語指示付き）をOllamaに渡し、回答生成をリクエストします。
10. **[中断チェックポイント3 (ベストエフォート)]** Ollamaの処理はアトミックな場合が多いため、MVPではOllamaの応答待ちは中断不可でも許容します。
11. Ollamaから回答が返却されたら、回答テキストと出典情報（検索された文書のファイル名）を `st.session_state['chat_history']` に追加します。
12. `st.session_state['app_state']` を `'idle'` に戻します。
13. UI層が `st.session_state` の変更を検知し、チャット履歴を更新して新しい回答を表示します。

---

### 3. 画面仕様

#### 3.1. 画面一覧

| 画面ID | 画面名 | 概要 |
| :--- | :--- | :--- |
| SCR-01 | メイン画面 | ユーザーが質問を入力し、AIからの回答と出典を確認する主要画面。 |
| SCR-02 | 設定画面 | ナレッジソースとなるフォルダの指定や、文書インデックスの管理を行う画面。 |

#### 3.2. メイン画面 (SCR-01) 詳細

##### 3.2.1. ワイヤーフレーム

**通常時:**

```text
+-----------------------------------------------------------------+
| ローカルナレッジアシスタント v0.1                                |
+-----------------------------------------------------------------+
| [設定]                                                          |
|-----------------------------------------------------------------|
| ▲                                                               |
| | **あなた:** 在宅勤務の交通費精算について教えて               |
| |                                                               |
| | **アシスタント:** |
| | 在宅勤務の際の交通費は、業務に必要な場合に限り...             |
| | **出典:** - 経費精算規定_ver3.pdf                             |
| |                                                               |
| ▼                                                               |
|-----------------------------------------------------------------|
| > [ここに質問を入力してください]                         [送信] |
+-----------------------------------------------------------------+
```

**Q&A処理中:**

```text
+-----------------------------------------------------------------+
| ローカルナレッジアシスタント v0.1                                |
+-----------------------------------------------------------------+
| [設定]                                                          |
|-----------------------------------------------------------------|
| ▲                                                               |
| | **あなた:** 在宅勤務の交通費精算について教えて               |
| |                                                               |
| | **アシスタント:** (回答生成中...)                            |
| |                                                               |
| ▼                                                               |
|-----------------------------------------------------------------|
| > [（入力不可）]                                   [キャンセル] |
+-----------------------------------------------------------------+
```

##### 3.2.2. コンポーネント仕様

| No | コンポーネントID | コンポーネント名 | UIライブラリ部品 (Streamlit) | 表示内容 / ラベル | アクション / 動作仕様 |
| :- | :--- | :--- | :--- | :--- | :--- |
| 1 | **SCR01-H01** | **ヘッダータイトル** | `st.header` | `ローカルナレッジアシスタント v0.1` | 静的表示。アプリケーションの名称とバージョンを示す。 |
| 2 | **SCR01-N01** | **ナビゲーション（サイドバー）** | `st.sidebar.radio` | メイン/設定選択 | サイドバーでページ選択。設定を選択すると設定画面 (SCR-02) を表示する。 |
| 3 | **SCR01-A01** | **チャット履歴エリア** | `st.container` | ユーザーとアシスタントの対話履歴。 | ・`st.session_state['chat_history']` で対話履歴を保持し、再描画時に表示。  ・履歴がエリアの高さを超える場合は、縦スクロールバーを表示。  ・最新のメッセージが最下部に表示されるように自動スクロールする。 |
| 4 | **SCR01-M01** | **ユーザーメッセージ** | `st.markdown` | `**あなた:**`  `(ユーザーが入力した質問テキスト)` | チャット履歴エリア内に表示される。 |
| 5 | **SCR01-M02** | **アシスタントメッセージ** | `st.markdown` | `**アシスタント:**`  `(LLMが生成した回答テキスト)` | チャット履歴エリア内に表示される。 |
| 6 | **SCR01-M03** | **出典表示** | `st.markdown` | `**出典:**`  `- (ファイル名1)` | アシスタントメッセージの下に表示される。回答の根拠となった文書のファイル名をリスト形式で表示する。 |
| 7 | **SCR01-I01** | **質問入力ボックス** | `st.text_input` | プレースホルダー: `ここに質問を入力してください` | ・ユーザーからのテキスト入力を受け付ける。  ・Enterキー押下で、送信ボタン (SCR01-B02) のクリックと同じアクションを実行する。  ・`st.session_state['app_state']` が `'processing_qa'` の間は無効化（入力不可）される。  ・**キーボードの上矢印キー**が押されたイベントを検知し、`st.session_state['prompt_history']` から過去の入力を遡って表示する。  ・**キーボードの下矢印キー**で新しい履歴に進み、最後まで進むと現在入力中のプロンプトに戻る。 |
| 8 | **SCR01-B02** | **送信ボタン** | `st.button` | `送信` | ・`st.session_state['app_state']` が `'idle'` の場合に表示。  ・クリックすると、バックエンド処理を開始し、`st.session_state['app_state']` を `'processing_qa'` に変更する。  ・質問受信時にタイムスタンプを記録し、送信した質問を `st.session_state['prompt_history']` に追加する。 |
| 9 | **SCR01-B03** | **キャンセルボタン (Q&A)** | `st.button` | `キャンセル` | ・`st.session_state['app_state']` が `'processing_qa'` の場合に表示。  ・クリックすると、`st.session_state['cancel_requested']` を `True` に設定する。  ・バックエンド処理は、このフラグを定期的に確認し、`True` の場合は処理を中断する。 |

---

#### 3.3. 設定画面 (SCR-02) 詳細

##### 3.3.1. ワイヤーフレーム

**通常時:**

```text
+------------------------------------------------------+
| 設定                                         [×]    |
+------------------------------------------------------+
| **モデル設定** |
| LLMモデル: [llama3:8b ▼]                           |
| 埋め込みモデル: [nomic-embed-text ▼]               |
|------------------------------------------------------|
| **ナレッジフォルダ設定** |
| C:\Users\YourName\Documents\社内規定         [変更]  |
|------------------------------------------------------|
| **データベース設定** |
| ベクトルストアパス: [./data/chroma_db       ]        |
|                                        [設定を保存]  |
|------------------------------------------------------|
| **インデックス** |
| 最終更新: 2025/08/30 10:00                          |
|                                        [今すぐ更新]  |
|------------------------------------------------------|
|                                              [閉じる]|
+------------------------------------------------------+
```

**インデックス作成処理中:**

```text
+------------------------------------------------------+
| 設定                                         [×]    |
+------------------------------------------------------+
| **モデル設定** |
| LLMモデル: [llama3:8b ▼]                           |
| 埋め込みモデル: [nomic-embed-text ▼]               |
|------------------------------------------------------|
| **ナレッジフォルダ設定** |
| C:\Users\YourName\Documents\社内規定         [変更]  |
|------------------------------------------------------|
| **データベース設定** |
| ベクトルストアパス: [./data/chroma_db       ]        |
|                                        [設定を保存]  |
|------------------------------------------------------|
| **インデックス** |
| 処理中... (5/50 ファイル完了)                        |
| [■■■■■□□□□□□□□□□□□□□□] [キャンセル]             |
|------------------------------------------------------|
|                                              [閉じる]|
+------------------------------------------------------+
```

##### 3.3.2. コンポーネント仕様

| No | コンポーネントID | コンポーネント名 | UIライブラリ部品 (Streamlit) | 表示内容 / ラベル | アクション / 動作仕様 |
| :- | :--- | :--- | :--- | :--- | :--- |
| 1 | **SCR02-H01** | **ヘッダータイトル** | `st.header` | `設定` | 静的表示。画面の目的を示す。 |
| 2 | **SCR02-B01** | **画面を閉じるボタン** | `st.button` | `×` | クリックすると、設定画面を非表示にし、メイン画面に戻る。 |
| 3 | **SCR02-S01** | **モデル設定セクション** | `st.subheader` | `モデル設定` | 静的表示。セクションタイトル。 |
| 4 | **SCR02-L01** | **LLMモデル選択** | `st.selectbox` | `LLMモデル` | インストール済みのOllamaモデルから選択。起動時にOllama APIから利用可能モデル一覧を取得し、選択肢として表示。デフォルト: `llama3:8b` |
| 5 | **SCR02-E01** | **埋め込みモデル選択** | `st.selectbox` | `埋め込み（ベクトル変換）用モデル` | 利用可能な埋め込みモデルから選択。選択肢: `nomic-embed-text`, `mxbai-embed-large`, `all-minilm`, `snowflake-arctic-embed` |
| 6 | **SCR02-S02** | **データベース設定セクション** | `st.subheader` | `データベース設定` | 静的表示。セクションタイトル。 |
| 7 | **SCR02-I02** | **ベクトルストアパス入力** | `st.text_input` | `ベクトルストアパス` | ChromaDBデータベースの保存先パスを表示・編集可能。デフォルト: `./data/chroma_db` |
| 8 | **SCR02-B02** | **設定保存ボタン** | `st.button` | `設定を保存` | モデル・データベース設定をconfig.jsonに保存。保存後は再起動が必要である旨を表示。 |
| 9 | **SCR02-S03** | **ナレッジフォルダ設定セクション** | `st.subheader` | `ナレッジフォルダ設定` | 静的表示。セクションタイトル。 |
| 10 | **SCR02-D01** | **現在のフォルダパス表示** | `st.text_input` (disabled) | 現在設定されているフォルダの絶対パス | ・読み取り専用で現在の設定値を表示する。  ・設定値は `st.session_state['config']` から読み込む。 |
| 11 | **SCR02-B03** | **フォルダ変更ボタン** | `st.button` | `変更` | ・クリックすると、OSのフォルダ選択ダイアログを開く。  ・新しいフォルダを選択後、`st.session_state['config']` を更新し、`config.json` ファイルに保存する。  ・フォルダ変更後、「インデックスの再構築が必要です」というメッセージ (`st.info`) を表示する。 |
| 12 | **SCR02-S04** | **インデックスセクション** | `st.subheader` | `インデックス` | 静的表示。セクションタイトル。 |
| 13 | **SCR02-D02** | **最終更新日時表示** | `st.markdown` | `最終更新: YYYY/MM/DD HH:mm` | `st.session_state['config']` に保存された最終更新日時を表示する。 |
| 14 | **SCR02-B04** | **インデックス更新ボタン** | `st.button` | `今すぐ更新` | ・`st.session_state['app_state']` が `'idle'` の場合に表示。  ・クリックすると、バックエンド処理を開始し、`st.session_state['app_state']` を `'processing_indexing'` に変更する。 |
| 15 | **SCR02-P01** | **進捗表示エリア** | `st.container` | `処理中... (X/Y ファイル完了)`  `st.progress(value)` | ・`st.session_state['app_state']` が `'processing_indexing'` の場合に表示。  ・インデックス作成処理のループ内で、ファイル処理ごとにUIを更新し、進捗状況をリアルタイムに表示する。 |
| 16 | **SCR02-B05** | **キャンセルボタン (インデックス)** | `st.button` | `キャンセル` | ・`st.session_state['app_state']` が `'processing_indexing'` の場合に表示。  ・クリックすると、`st.session_state['cancel_requested']` を `True` に設定する。  ・バックエンド処理は、このフラグを定期的に確認し、`True` の場合は処理を中断する。 |
| 17 | **SCR02-B06** | **閉じるボタン (フッター)** | `st.button` | `閉じる` | クリックすると、設定画面を非表示にし、メイン画面に戻る (SCR02-B01 と同じ機能)。 |

##### 3.3.3. LLMモデル動的取得仕様

**LLMモデル選択の実装詳細:**

| 項目 | 仕様 |
| :--- | :--- |
| **モデル一覧取得** | ・設定画面表示時にOllama API (`http://localhost:11434/api/tags`) にGETリクエストを送信  ・レスポンスJSONから利用可能モデル一覧を取得  ・APIが利用できない場合はフォールバックリスト `['llama3:8b', 'llama3:70b', 'mistral:latest', 'codellama:13b', 'gemma:2b', 'gemma:7b']` を使用 |
| **選択肢の表示** | ・取得したモデル一覧を `st.selectbox` の選択肢として設定  ・現在の設定値 (`config.ollama_model`) が一覧に含まれる場合は初期選択として設定  ・含まれない場合は一覧の最初のモデルを初期選択 |
| **エラーハンドリング** | ・Ollama接続失敗時: 警告メッセージ表示（⚠️ Ollama接続失敗 - デフォルトモデル一覧を表示）  ・接続成功時: 成功メッセージ表示（✅ Ollama接続成功 (N)モデル利用可能）  ・予期しないエラー時: 手動入力フィールドにフォールバック |
| **モデル情報表示** | ・選択されたモデルの詳細情報（サイズ、最終更新日等）を表示（オプション）  ・モデルが大きい場合はメモリ使用量の警告を表示 |

**API仕様 (Ollama Tags API):**

```json
// GET http://localhost:11434/api/tags
// レスポンス例:
{
  "models": [
    {
      "name": "llama3:8b",
      "modified_at": "2024-08-30T10:00:00Z",
      "size": 4661224192
    },
    {
      "name": "codellama:latest", 
      "modified_at": "2024-08-29T15:30:00Z",
      "size": 3826793472
    }
  ]
}
```

---

### 4. 共通仕様

| 項目 | 仕様 |
| :--- | :--- |
| **状態管理** | アプリケーションの状態は `st.session_state` で一元管理する。  ・`st.session_state['app_state']`: アプリケーションの主要な状態 (`'idle'`, `'processing_qa'`, `'processing_indexing'`)  ・`st.session_state['cancel_requested']`: 中断要求の有無 (`True` / `False`)  ・`st.session_state['chat_history']`: 対話履歴のリスト  ・`st.session_state['config']`: ナレッジフォルダパスや最終更新日時などの設定情報  ・`st.session_state['prompt_history']`: ユーザーが送信した質問の履歴リスト  ・`st.session_state['history_cursor']`: プロンプト履歴を遡るためのカーソル位置 |
| **処理の中断メカニズム** | ・長時間処理（Q&A、インデックス作成）のループ内部で、定期的に `st.session_state['cancel_requested']` をチェックする。  ・フラグが `True` の場合、ループを `break` し、処理を安全に終了する。  ・中断後、チャット履歴や設定画面に「処理が中断されました」というメッセージを表示し、`app_state` を `'idle'` に、`cancel_requested` を `False` に戻す。 |
| **パフォーマンス目標** | ・**目標値:** ユーザーが質問を送信してから、回答が画面に表示されるまでの時間は**30秒以内**を目標とする（ベストエフォート）。  ・**測定方法:** 質問送信時と回答表示完了時にタイムスタンプ (`datetime.now()`) を取得し、その差分をコンソールログに出力する機能を実装する。「`[PERF] Response time: X.XX seconds`」の形式で出力すること。 |
| **永続化** | ・ナレッジフォルダのパス、LLMモデル設定、埋め込みモデル設定、データベースパス、インデックスの最終更新日時などの設定情報は、`st.session_state['config']` の内容と同期させ、変更があるたびにローカルの `config.json` ファイルに保存する。  ・チャット履歴およびプロンプト履歴は、アプリケーション実行中のセッション内でのみ保持する。  ・埋め込みモデル変更時は次元数の互換性を自動チェックし、必要に応じてコレクションを再作成する。再起動は不要。 |
| **UIフィードバック** | ・**ロード中:** 時間のかかる処理の実行中は、ユーザーに処理中であることが分かるようにインジケータ（プログレスバーやスピナー）を表示する (`st.spinner`, `st.progress`)。  ・**エラー:** ファイル読み込み失敗、LLM応答エラー等の場合は、その内容をユーザーに分かりやすく通知する (`st.error`)。  ・**情報:** 処理に関する注意喚起や補足情報（例: フォルダ変更後のインデックス更新推奨）を表示する (`st.warning`, `st.info`)。 |
| **埋め込みモデル動的対応** | ・**自動次元数検出:** 埋め込みモデルの次元数マッピング（`nomic-embed-text`: 768次元、`mxbai-embed-large`: 1024次元等）を内蔵し、未知のモデルは実際の出力から自動検出する。  ・**互換性チェック:** インデクサー初期化時とモデル変更時に、現在のモデルの次元数と既存コレクションの次元数を比較し、不整合を検出する。  ・**自動コレクション再作成:** 次元数不整合を検出した場合は、既存データを削除して新しい次元数に対応したコレクションを自動作成する。ユーザーには警告メッセージで通知する。  ・**シームレスな切り替え:** 設定画面でのモデル変更時に再起動は不要で、バックグラウンドで自動的に互換性調整が実行される。 |

---

### 5. 実装詳細

#### 5.1. ファイル・ディレクトリ構造

本アプリケーションの実装は、以下のファイルおよびディレクトリ構造に従うこと。これにより、責務の分離とコードの可読性を確保する。

```text
local-knowledge-agent/
│
├── .streamlit/
│   └── config.toml           # Streamlitのテーマや設定（オプション）
│
├── data/
│   ├── chroma_db/            # ChromaDBの永続化データが保存されるディレクトリ
│   └── config.json           # ナレッジフォルダパス等の設定を保存するファイル
│
├── src/
│   ├── __init__.py
│   ├── logic/
│   │   ├── __init__.py
│   │   ├── indexing.py              # ドキュメントの読み込み、分割、インデックス作成ロジック
│   │   ├── qa.py                    # Q&Aパイプラインの構築と実行ロジック
│   │   └── ollama_model_service.py  # Ollama APIとの通信、モデル情報取得ロジック
│   │
│   └── ui/
│       ├── __init__.py
│       ├── main_view.py      # メイン画面(SCR-01)のUIコンポーネントとロジック
│       └── settings_view.py  # 設定画面(SCR-02)のUIコンポーネントとロジック
│
├── app.py                    # Streamlitアプリケーションのエントリーポイント
├── requirements.txt          # 依存ライブラリ一覧
└── README.md                 # プロジェクトの説明書
```

**各ファイル/ディレクトリの責務:**

* `app.py`: アプリケーションの起動、`st.session_state` の初期化、UIモジュール（`main_view`, `settings_view`）の呼び出しなど、全体的な制御を行う。
* `src/logic/indexing.py`: ファイルの読み込み、チャンクへの分割、ベクトル化、ChromaDBへの保存といった、インデックス作成に関連する全てのバックエンド処理をカプセル化する。埋め込みモデル次元数の動的対応機能も含む。
* `src/logic/qa.py`: ベクトルDBからの文書検索、プロンプトの構築、Ollamaへの問い合わせといった、Q&A応答生成に関連する全てのバックエンド処理をカプセル化する。
* `src/logic/ollama_model_service.py`: Ollama APIとの通信を担当し、利用可能モデルの動的取得、接続確認、エラーハンドリングとフォールバック機能を提供する。
* `src/ui/main_view.py`: メイン画面のUIレイアウトと、ユーザー操作に応じたバックエンドロジック（`qa.py`）の呼び出しを担当する。
* `src/ui/settings_view.py`: 設定画面のUIレイアウトと、ユーザー操作に応じたバックエンドロジック（`indexing.py`）の呼び出しを担当する。
* `data/`: ユーザーデータや設定など、アプリケーションが実行中に生成・利用する全ての永続化データを格納する。このディレクトリはGitの管理対象外（`.gitignore`に記載）とすることが望ましい。

#### 5.2. 日本語回答制御機能

**概要:** 全ての質問に対して日本語で回答を生成するようLLMを制御する機能。

**実装仕様:**

* **プロンプト制御:** RAGパイプラインで生成するプロンプトの先頭に「日本語で回答してください。」という指示を含める。
* **データモデル拡張:** QAResultクラスに`response_language: str = "ja"`フィールドを追加し、常に日本語固定とする。
* **一貫性保証:** 通常のRAGモードと直接QAモード（文書なし）の両方で日本語指示を適用する。

**技術実装:**

```python
# プロンプトテンプレート例
qa_prompt_template = """日本語で回答してください。

以下のコンテキスト情報を参考にして、ユーザーの質問に正確で有用な回答を提供してください。

【コンテキスト情報】
{context}

【質問】
{query}

【回答】"""

# QAResultモデル
@dataclass
class QAResult:
    query: str
    answer: str
    sources: List[Dict[str, Any]]
    context: str
    response_language: str = "ja"  # 日本語固定
    ...
```

**検証方法:**

* 英語、日本語、その他言語での質問に対して全て日本語回答を生成することを自動テストで確認
* ドキュメントが存在しない場合でも日本語回答制御が機能することを確認

#### 5.3. 対応ドキュメント拡張仕様

**概要:**
Markdown (.md) およびMicrosoft Office系のファイル形式（Word .docx, Excel .xlsx, PowerPoint .pptx）の読み込みとインデックス作成に対応する。

**技術選定:**

* **主要ライブラリ:** `docling` ライブラリを採用する（IBM Research開発、2025年1月最新版）。
* **補助ライブラリ:** 従来の`python-docx`, `openpyxl`, `python-pptx`も併用し、フォールバック処理に使用。
* **採用理由:**
  * Doclingは2025年のRAG処理におけるOffice系ドキュメント処理の新標準
  * LangChain統合対応済み（`from langchain_community.document_loaders import DoclingPDFLoader`）
  * セマンティックチャンキング、表構造保持、レイアウト認識などRAG最適化機能内蔵
  * MIT ライセンスで商用利用可能
  * PDF、DOCX、PPTX、XLSX、HTML、画像など多形式を統一処理

**実装仕様:**

* **依存関係:** `requirements.txt` に以下を追加する：
  * `docling` - 主要なOffice系ドキュメント処理
  * `python-docx`, `openpyxl`, `python-pptx` - フォールバック処理用
  * `markdown` - Markdownファイル処理用
  
* **ファイル形式別処理:** `src/logic/indexing.py` 内でファイル拡張子に応じた最適なローダーを選択：
  * **.md**: `UnstructuredMarkdownLoader`
  * **.docx/.xlsx/.pptx**: `DoclingLoader` (主要)、従来ライブラリ (フォールバック)
  * **.pdf**: 既存の`PyPDFLoader`を継続使用
  * **.txt**: 既存の処理を継続

* **セマンティックチャンキング:** Doclingの内蔵チャンキング機能を活用し、表構造やレイアウト情報を保持したチャンク分割を実行する。

    ```python
    # Doclingローダーの使用例
    from langchain_community.document_loaders import DoclingLoader
    from langchain_community.document_loaders import UnstructuredMarkdownLoader

    def _load_office_document(self, file_path: Path) -> List[Document]:
        """Office系ドキュメントの読み込み（Docling使用）"""
        try:
            loader = DoclingLoader(str(file_path))
            documents = loader.load()
            return documents
        except Exception as e:
            # フォールバック: 従来ライブラリを使用
            return self._load_office_document_fallback(file_path)
    ```

* **エラーハンドリング:** 主要ライブラリでの処理失敗時は、自動的にフォールバック処理を実行し、処理継続性を確保する。

#### 5.4. 埋め込みモデル動的次元数対応仕様

**概要:**
埋め込みモデルの切り替え時に次元数の違いを自動検出し、ChromaDBコレクションを適切に管理する機能。

**技術仕様:**

* **次元数マッピング:** `ChromaDBIndexer`クラス内で、サポートする埋め込みモデルの次元数を定義：
  ```python
  EMBEDDING_DIMENSIONS = {
      "nomic-embed-text": 768,
      "mxbai-embed-large": 1024,
      "all-minilm": 384,
      "snowflake-arctic-embed": 1024
  }
  ```

* **自動次元数検出:** 未知のモデルに対しては、実際の埋め込みベクトルを生成してサイズを動的に取得。

* **互換性チェック機能:**
  ```python
  def check_embedding_dimension_compatibility(self) -> dict:
      """次元数互換性をチェックし、結果を詳細な辞書で返す"""
      # 現在のモデルの次元数を取得
      # 既存コレクションの次元数を確認
      # 不整合の有無とアクション必要性を判定
  ```

* **自動コレクション再作成:**
  * 次元数不整合検出時に既存コレクションを削除
  * 新しい次元数に対応したコレクションを作成
  * ユーザーに影響（既存データ削除）を明確に通知

* **シームレスなモデル変更:**
  ```python
  def update_embedding_model(self, new_model: str) -> bool:
      """埋め込みモデルを変更し、必要に応じてコレクション再作成"""
      # 新モデルの次元数を確認
      # 現在のコレクションとの互換性をチェック
      # 不整合時は自動的にコレクション再作成
      # 成功/失敗を返し、UIに反映
  ```

**実装上の考慮点:**

* **エラー時の復旧:** モデル変更失敗時は元のモデルに自動復旧
* **ログ出力:** 次元数検出、コレクション再作成の詳細をログに記録
* **パフォーマンス:** 初期化時の次元数チェックは最小限に抑制
* **ユーザビリティ:** 設定画面で再作成の必要性と影響を事前に表示

#### 5.6. Ollamaモデル管理機能仕様

**概要:**
アプリケーション起動時にOllama APIからモデル一覧を動的取得し、埋め込みモデルの選択肢を動的にフィルタリングする機能。

**技術仕様:**

* **起動時モデル一覧取得:**
  ```python
  def fetch_available_models() -> dict:
      """Ollama APIからモデル一覧を取得"""
      # GET /api/tags リクエストで全モデルを取得
      # モデル名、サイズ、更新日時等の詳細情報を含む
      # 接続エラー時はフォールバック対応
  ```

* **埋め込みモデルフィルタリング:**
  ```python
  def filter_embedding_models(all_models: list) -> list:
      """埋め込みモデルのみをフィルタリング"""
      EMBEDDING_MODEL_PATTERNS = [
          "embed", "embedding", "nomic", "mxbai", "minilm", "arctic"
      ]
      # パターンマッチングで埋め込みモデルを識別
      # サポート対象の次元数マッピングと突合
  ```

* **必須モデルチェック:**
  ```python
  def validate_required_models(available_models: list) -> dict:
      """必須モデルの存在確認"""
      required = ["nomic-embed-text"]
      missing = [model for model in required if model not in available_models]
      return {
          "is_valid": len(missing) == 0,
          "missing_models": missing,
          "install_commands": [f"ollama pull {model}" for model in missing]
      }
  ```

* **設定画面での動的表示:**
  ```python
  # settings_view.py内でハードコード選択肢を動的取得に変更
  available_embedding_models = st.session_state.get('available_embedding_models', [])
  selected_embedding_model = st.selectbox(
      "埋め込みモデル",
      options=available_embedding_models,
      index=available_embedding_models.index(current_model) if current_model in available_embedding_models else 0
  )
  ```

**実装上の考慮点:**

* **エラーハンドリング:** Ollama接続失敗時は警告表示とフォールバック選択肢を提供
* **キャッシュ機構:** セッション内でのモデル一覧キャッシュで API呼び出し最小化
* **バリデーション:** 選択されたモデルの存在確認と互換性チェック
* **ユーザビリティ:** モデル一覧取得中の進捗表示とエラー時の対処方法案内

#### 5.7. チャット入力補助機能仕様

**概要:**
ユーザーが質問入力ボックスでキーボードの上下矢印キーを押すことで、過去に送信した質問（プロンプト）の履歴を簡単に呼び出し、再利用できる機能。

**実装仕様:**

* **履歴の保存:** ユーザーが「送信」ボタンを押した際、入力された質問を`st.session_state['prompt_history']`リストの末尾に追加する。重複する質問もそのまま追加する。
* **状態管理:** 以下のセッションステート変数を使用する。
  * `st.session_state['prompt_history']`: 送信済み質問のリスト。
  * `st.session_state['history_cursor']`: 履歴リストを指すカーソル。初期値は `len(prompt_history)`。
  * `st.session_state['current_input']`: ユーザーが履歴を呼び出す前に入力していたテキスト。
* **キーイベントの検知:**
  * Streamlitの標準機能ではキーイベントを直接取得できないため、サードパーティのカスタムコンポーネント（例: `streamlit-keyup`）の利用を検討する。
  * コンポーネントを利用し、上矢印キー（`ArrowUp`）と下矢印キー（`ArrowDown`）の押下を検知する。
* **動作ロジック:**
    1. **上矢印キー押下時:**
        * カーソルが履歴の先頭 (`0`) より大きい場合、`history_cursor` を1つ減らす。
        * 現在の入力内容が履歴と異なる場合、`current_input` に保存する。
        * `prompt_history[history_cursor]` の内容を入力ボックスに表示する。
    2. **下矢印キー押下時:**
        * カーソルが履歴の末尾 (`len(prompt_history)`) より小さい場合、`history_cursor` を1つ増やす。
        * カーソルが履歴の末尾に達した場合、保存していた `current_input` を入力ボックスに戻す。
        * それ以外の場合は、`prompt_history[history_cursor]` の内容を入力ボックスに表示する。
    3. **テキスト入力時:** ユーザーが入力ボックスの内容を手動で変更した場合、`history_cursor` を `len(prompt_history)` にリセットし、`current_input` をクリアする。これにより、履歴表示モードが解除され、通常の入力モードに戻る。
